{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc056f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import isodate\n",
    "\n",
    "\n",
    "#set your API key\n",
    "api_key = \"---\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "#here is request parameters\n",
    "publishedAfter = datetime.strptime(\"2006-01-01T00:00:00Z\", \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "publishedBefore = datetime.strptime(\"2023-05-16T23:59:59Z\", \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "max_results = 50\n",
    "total_results = 0\n",
    "page_token = \"\"\n",
    "file_counter = 1\n",
    "video_counter = 0\n",
    "#Sets the next \"publishedAfter\" date for the search, considering the maximum results per day\n",
    "while publishedAfter < publishedBefore:\n",
    "    next_publishedAfter = publishedAfter + timedelta(days=1)\n",
    "    if next_publishedAfter > publishedBefore:\n",
    "        next_publishedAfter = publishedBefore\n",
    "#Executes a search request to the YouTube API to retrieve videos related to \"climate change\" within the specified date range\n",
    "    search_response = youtube.search().list(\n",
    "        q=\"climate change\",\n",
    "        type=\"video\",\n",
    "        part=\"id\",\n",
    "        order='title',\n",
    "        relevanceLanguage='en',\n",
    "        publishedAfter=publishedAfter.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        publishedBefore=next_publishedAfter.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        maxResults=max_results,\n",
    "        pageToken=page_token\n",
    "    ).execute()\n",
    "#Checks if there are any search results. If not, breaks the loop\n",
    "    if 'items' not in search_response:\n",
    "        break\n",
    "#Retrieves the search results and updates the total number of results\n",
    "    search_results = search_response.get(\"items\", [])\n",
    "    total_results += len(search_results)\n",
    "\n",
    "#Initializes the CSV file for writing the search results if it's the first video in the file\n",
    "    tags = []\n",
    "    if video_counter == 0:\n",
    "        filename = f\"May/search_results_{file_counter}.csv\"\n",
    "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)          \n",
    "# write header of csv file\n",
    "            writer.writerow(\n",
    "                ['title', 'channelId', 'channelTitle', 'videoId', 'tags', 'description', 'publishDate', 'url',\n",
    "                 'thumbnail_high', 'duration', 'viewCount', 'likeCount', 'dislikeCount', 'commentCount'])\n",
    "            for result in search_results:\n",
    "                video_id = result[\"id\"][\"videoId\"]\n",
    "\n",
    "# After get Video ID, the code makes an API request to retrieve the video details using the youtube.videos().list() method\n",
    "                video_response = youtube.videos().list(\n",
    "                    part=\"contentDetails,statistics, snippet\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "#It checks if there are video items in the response and continues to the next iteration if not\n",
    "                if 'items' not in video_response:\n",
    "                    continue\n",
    "                video_results = video_response.get(\"items\", [])\n",
    "\n",
    "                if not video_results:\n",
    "                    continue\n",
    "\n",
    "                video = video_results[0]\n",
    "\n",
    "                title = video[\"snippet\"][\"title\"]\n",
    "                channel_id = video['snippet']['channelId']\n",
    "                channel_title = video['snippet']['channelTitle']\n",
    "                tags = video['snippet'].get(\"tags\", None)\n",
    "                description = video[\"snippet\"][\"description\"]\n",
    "                publish_date = video[\"snippet\"][\"publishedAt\"]\n",
    "                url = \"https://www.youtube.com/watch?v=\" + video_id\n",
    "                thumbnail_high = video['snippet']['thumbnails']['high'].get('url', 0)\n",
    "                duration_str = video['contentDetails'].get('duration', '')\n",
    "                duration = isodate.parse_duration(duration_str)\n",
    "\n",
    "                # Convert to a timedelta object\n",
    "                duration_timedelta = timedelta(seconds=duration.total_seconds())\n",
    "                view_count = video['statistics'].get('viewCount', 0)\n",
    "                like_count = video['statistics'].get('likeCount', 0)\n",
    "                dislike_count = video['statistics'].get('dislikeCount', 0)\n",
    "                commentCount = video['statistics'].get('commentCount', 0)\n",
    "#according to the header, write rows\n",
    "                writer.writerow([title, channel_id, channel_title, video_id, tags, description,\n",
    "                                 publish_date, url, thumbnail_high, duration_timedelta, view_count,\n",
    "                                 like_count, dislike_count, commentCount])\n",
    "#The code then increments the counters video_counter and file_counter based on the number of videos processed\n",
    "#If video_counter reaches 500, it increments file_counter and resets video_counter to 0(that means if the rows reach to 500, it will create new csv file and start write)\n",
    "\n",
    "                video_counter += 1\n",
    "                if video_counter == 500:\n",
    "            \n",
    "#This part of the code handles writing the video details to a CSV file when the video_counter reaches 500\n",
    "                    file_counter += 1\n",
    "                    video_counter = 0\n",
    "    else:\n",
    "        filename = f\"May/search_results_{file_counter}.csv\"\n",
    "        with open(filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for result in search_results:\n",
    "                video_id = result[\"id\"][\"videoId\"]\n",
    "\n",
    "                # Get video details\n",
    "                video_response = youtube.videos().list(\n",
    "                    part=\"contentDetails,statistics, snippet\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "\n",
    "                if 'items' not in video_response:\n",
    "                    continue\n",
    "\n",
    "                video_results = video_response.get(\"items\", [])\n",
    "\n",
    "                if not video_results:\n",
    "                    continue\n",
    "\n",
    "                video = video_results[0]\n",
    "\n",
    "                title = video[\"snippet\"][\"title\"]\n",
    "                channel_id = video['snippet']['channelId']\n",
    "                channel_title = video['snippet']['channelTitle']\n",
    "                tags = video['snippet'].get('tags', None)\n",
    "                description = video[\"snippet\"][\"description\"]\n",
    "                publish_date = video[\"snippet\"][\"publishedAt\"]\n",
    "                url = \"https://www.youtube.com/watch?v=\" + video_id\n",
    "                thumbnail_high = video['snippet']['thumbnails']['high'].get('url', 0)\n",
    "                duration_str = video['contentDetails'].get('duration', '')\n",
    "                duration = isodate.parse_duration(duration_str)\n",
    "\n",
    "                # Convert to a timedelta object\n",
    "                duration_timedelta = timedelta(seconds=duration.total_seconds())\n",
    "                view_count = video['statistics'].get('viewCount', 0)\n",
    "                like_count = video['statistics'].get('likeCount', 0)\n",
    "                dislike_count = video['statistics'].get('dislikeCount', 0)\n",
    "                commentCount = video['statistics'].get('commentCount', 0)\n",
    "                writer.writerow([title, channel_id, channel_title, video_id, tags, description,\n",
    "                                 publish_date, url, thumbnail_high, duration_timedelta, view_count,\n",
    "                                 like_count, dislike_count, commentCount])\n",
    "\n",
    "                video_counter += 1\n",
    "                if video_counter == 500:\n",
    "                    file_counter += 1\n",
    "                    video_counter = 0\n",
    "# Check if there are more pages of results\n",
    "#If there are more pages, it updates the publishedAfter date and page_token for the next iteration of the loop\n",
    "    if 'nextPageToken' not in search_response:\n",
    "        publishedAfter = next_publishedAfter\n",
    "        page_token = \"\"\n",
    "    else:\n",
    "        page_token = search_response['nextPageToken']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
